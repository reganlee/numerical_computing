\lab{Application}{K-Nearest Neighbors}{K-Nearest Neighbors}
\label{Ch:KNN}

\objective{Implement a K-Nearest Neighbors classification algorithm using the Nearest Neighbor search from the previous section}

\section*{Classification}

A common problem is correctly classifying data.  Suppose that you have a ten marbles.  Suppose further that five of your marbles are blue with a green stripe and the other five are red with a purple stripe.  If your friend gave you an eleventh marble that was blue with a brown stripe, which group would you put it in?  Probably you would include it with the other blue marbles.  What if the marble that your friend gave you was blue with a purple stripe?  This marble shares characteristics with both groups.  Where you end up grouping it will depend on which characteristics are most important to you.

This is the intuitive classfication problem.  If we have data that is grouped into some sets for us, what set do we put new data into?  Classification has myriad and sundry applications.  In this lab we will use it in an optical character recognition application.

\section*{Nearest Neighbor Classification}

We now more formally describe the classification problem and explain the nearest neighbor classification algorithm.  Suppose that we have a collection of vectors $\{x_1, ..., x_m\}$ in $\R^n$ with corresponding labels $\{l_1, ..., l_k\}$ describing to which group each datum belongs.  This collection of vectors and labels is called our training set.  Each entry of a vector is called a feature, and $n$ is the size of our feature set.  For example, consider the following vectors in $\R^3$

\begin{center}
\begin{tabular}{cc}
$(2,0,0)$ & $1$ \\
$(3,0,0)$ & $1$ \\
$(0,3,0)$ & $2$ \\
$(0,2,0)$ & $2$ \\
$(\frac{1}{10},2,0)$ & $2$ \\
$(0,0,4)$ & $3$ \\
$(0,0,7)$ & $3$ \\
\end{tabular}
\end{center}

If we also have a metric on our space, we may determine the distance between all of these points.  If we are given a new datum and we wish to decide which of the three groups to include it in, one option is to choose the group to which it's closest neighbor belongs.  Let us use the euclidean metric to classify $(0,0,5)$ against our training set.  We see that the distance from $(0,0,5)$ and $(0,0,4)$ is only $1$, while the distance to the remaining points is at least $2$.  The label of $(0,0,4)$ is $3$, and so we assign $(0,0,5)$ the same label.

Now, what if we wished to classify $(\frac{5}{2},\frac{5}{2},0)$?  This presents a problem since this point is equidistant from the points in label $1$ and label $2$.  In such a case it is up to the programmer to decide how to break the tie.

\begin{problem}

Write a function that stores a training set (points and labels) and calculates the nearest neighbor of a new point and assigns it the corresponding label.

\end{problem}

An important consideration when using this algorithm is the metric that we choose to classify with.  For example, suppose we wished to classify people applying for a loan at a bank as `risky' or `safe.'  Further suppose that we know their age, education level, current debt, and how many credit cards they have.  We could encode their education level as integers between $0$ and $4$.

Suppose that Bob is 30 years old, well educated, has a debt of \$5000, and 3 credit cards.  Let's say that James is 18 years old, barely out of high school, \$5000 dollars in debt, and own 4 credit cards.  If we were to use the eucidean metric $d$ to measure how close Bob is to James, we would get a distance of
\[
d((30,4,5000,3),(18,0,5000,4)) = 12^2 + 4^2 + 1^2 = 161.
\]

However, if Alice is well 29 years old, well educated, has a debt of \$4900, and 3 credit cards, then her distance from Bob is
\[
d((30,4,5000,3),(29,4,4900,3)) = 1^2 + 100^2 = 100001.
\]

Is James closer to Bob than Alice?  Most Banks would say no.  In order to classify these individuals better, we need to measure distance differently.

\begin{problem}

Modify your solution to your previous problem to call a metric function to measure distance.  Write a metric function to determine distance using data from our bank example.

\end{problem}

\section*{K-Nearest Neighbor Classification}

Often we can improve the accuracy of a classifier by looking other points beside the nearest neighbor.  Instead, we may choose an arbitrary number $k$ and give the point to be classified the majority label in from it's $k$ nearest neighbors. 

There are pitfalls to this approact.  Consider a point that's closest neighbor has the label $0$.  If we only considered the nearest neighbor, then we would be finished.  However, what if the next $10$ nearest neighbors all had the label $1$?  Do you think that we should still classify the point as $0$?  What if the nearest point has a distance of $0.1$ and the next $10$ points have a distance of at least $100$?  The answer to these questions depend on the kind of data we are working with and the metric that we choose.  One should ensure to consider how to treat situations like this when working on classification algorithms.

\begin{problem}

Get the post office handwritten digit data set.  This contains a training set and a test set.  Each image is a $28 \times 28$ matrix of pixels with a label indicating which the number that was written.  Choose a metric to measure distance between pixels and use this metric to classify each image in the test set.  Return a report indicating how your classifier performs in terms of misclassifications.

A similar classification process is used by the United States Postal Service to automatically determine the zip code to send a letter to.

\end{problem}




















































