\lab{Algorithms}{Modified Gram-Schmidt (QR)}{QR decomposition}
\label{lab:QRdecomp}

\objective{Understand how the QR algorithm works and write your own implementation.}

The QR decomposition is used to represent any invertible square matrix as the matrix product of an orthogonal matrix, $Q$, and an upper triangular matrix, $R$.
This decomposition is useful in computing least squares and is part of a common method for finding eigenvalues (the QR algorithm).
We can succinctly state the QR decomposition in the following theorem.
\begin{theorem}
Let $A$ be an $m\times n$ matrix of rank $n$.  Then $A$ can be
factored into a product $Q R$, where $Q$ is an $m\times n$ matrix
with orthonormal columns and $R$ is a nonsingular $n \times n$ upper
triangular matrix.
\end{theorem}

\section*{Computing the QR Decomposition}
There are many methods for computing the QR factorization.
This lab will focus on the method that uses the Gram-Schmidt algorithm for orthogonalizing 
a full-rank matrix.
We use this algorithm to create $Q$.
Let $\{\x_i\}_{i=1}^n$ be a basis for the inner product space $V$.
Let \[ \q_1 = \frac{\x_1}{\norm{\x_1}}\] and define $\q_2,\q_3,\ldots,\q_n$ recursively by
$$ 
\q_{k} = \frac{\x_k - \p_{k-1}}{\|\x_k - \p_{k-1}\|}, \,\,\,\, k=2,\ldots,n,
$$
where
$$
\p_{k-1} = \sum_{i=1}^{k-1} \langle \q_i, \x_k\rangle \q_i,
$$
and $\p_0 = 0$. 
Then the set $\{\q_i\}_{i=1}^n$ is an orthonormal basis for $V$.

For the above algorithm, let $r_{j k} = \langle \q_j, \x_k\rangle$ when $j < k$ and
$r_{kk} = \|\x_k-\p_{k-1}\|$.
This can be written as
\begin{align*}
\x_1 &= r_{1 1} \q_1\\
\x_2 &= r_{1 2} \q_1 + r_{2 2} \q_2\\
\vdots \:\: &= \quad \vdots\\
\x_n &= r_{1 n} \q_1 + r_{2 n} \q_2 + \ldots + r_{n n} \q_{n},
\end{align*}
or in matrix form as
\[
[\x_1 \hspace{5mm} \x_2 \hspace{5mm} \cdots \hspace{5mm} \x_n]
=
[\q_1 \hspace{5mm} \q_2 \hspace{5mm} \cdots \hspace{5mm} \q_n]
\begin{bmatrix}
r_{1 1} & r_{1 2} & \cdots & r_{1 n}\\
0 & r_{2 2} & \cdots & r_{2 n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & r_{n n}
\end{bmatrix}.
\]
Hence if our original basis vectors $\{\x_i\}_{i=1}^n$ correspond to column
vectors of a matrix $A$, we can likewise write the resulting
orthonormal basis $\{\q_i\}_{i=1}^n$ as a matrix $Q$ of column
vectors.  Then we have that $A = Q R$, where $R$ is the above
nonsingular upper-triangular $n\times n$ matrix.
Numerically, the Gram Schmidt process can have problems due to
finite precision arithmetic. In some cases, rounding errors may cause
the resulting basis to fail to be orthonormal. To combat this, we
actually carry out a slightly revised algorithm called Modified Gram
Schmidt.  To do this, we compute $\q_1$ as before.  We then project
it out of each of the remaining original vectors
$\x_2,\x_3,\ldots,\x_n$ via
\[
\x_k := \x_k - \langle \q_1,\x_{k} \rangle \q_1,\quad k=2,\ldots,n.
\]
Then we compute $\q_2$ to be the unit vector of $\x_2$, that is,
\[
\q_2 = \frac{\x_2}{\|\x_2\|}.
\]
We repeat by projecting out $\q_2$ from the remaining vectors
$\x_3,\x_4,\ldots,\x_n$, and then continuing this process until
all $\q_i$ are obtained.

\begin{problem}
\label{prob:QR}
Write your own implementation of the QR decomposition.
Write a function \li{QR} that accepts as input a square matrix $A$ of full rank, 
and computes the QR decomposition, returning the matrices 
$Q$ and $R$ (which should be the same shape as $A$).
Be sure to use the numerically stable Modified Gram-Schmidt algorithm.
Also assume that there are no zeros on the main diagonal.

You can test that you have the right decomposition by verifying that $QR=A$ and $Q^T Q = I$.
\end{problem}

\begin{problem}
The QR decomposition gives a really nice way to calculate the magnitude of the determinant 
of a square matrix of full rank.
Write a function \li{QRDet} that accepts a square matrix of full rank as input, and returns
the magnitude of the determinant of that matrix. Use your QR decomposition to perform this
calculation.

You may check your work by comparing your results to those produce by \li{scipy.linalg.det}. 
\end{problem}

\section*{QR Decomposition in SciPy}
The linear algebra library in SciPy wraps the very efficient algorithms of LAPACK to calculate the QR decomposition.
In addition to being much faster, SciPy's QR decomposition is also much more general and can decompose non-square matrices.
\begin{lstlisting}
>>> import numpy as np
>>> from scipy import linalg as la
>>> A = np.random.rand(4,3)
>>> Q, R = la.qr(A)
>>> Q.dot(R) == A                      # there are False entries
>>> np.allclose(Q.dot(R), A)           # A = QR
>>> np.allclose(Q.T.dot(Q), np.eye(4)) # Q is indeed, orthogonal
\end{lstlisting}

In order to interpret the results correctly, we need to understand that the computer has limited precision (especially with floating point numbers).
This is why \li{Q.dot(R)} is not exactly equal to \li{A}.
We can see that the matrix product of $QR$ is very close to $A$.
This shows that indeed the product of $Q$ and $R$ is $A$.
Note also that $Q^T Q = I$.
This implies that the column vectors of $Q$ are orthonormal.

\section*{Solving Least Squares Problems}
The QR decomposition can be used to solve the linear least squares problem if $A$ is full rank.
If $A$ is less than full rank, then we have to calculate the least squares solution a little more creatively.
%MATLAB's least squares backslash operator is based off the QR decomposition.
%SciPy uses the SVD to solve least squares problems because, although it is slower, the algorithm is more numerically stable.
For large or ill-conditioned problems, the QR decomposition provides
a nice method for computing least squares solutions of
over-determined matrices.
Consider the least squares problem $Ax=b$.
We can approximate the solution with $\widehat x = (A^T A)^{-1}A^T b$.
Alternatively, we write the linear system as
\[ Q R x = b. \]
We then multiply both sides by $Q^T$, yielding
\[ R x = Q^T b. \]
Then $\widehat x = R^{-1} Q^T b$.

However, we can avoid calculating the inverse of $R$ (inverting a matrix is \emph{very expensive} computationally).
Since $R$ is a triangular matrix, we have a triangular system that we can solve much more efficiently.
SciPy includes a solver for triangular systems, \li{linalg.solve_triangular()}.
We approximate $x$ by solving the triangular system $Rx = Q^T b$.

\begin{problem}
Write a function \li{LeastSquares} that will accept a linear system (a matrix $A$ and a vector $b$) and solve the least squares problem.
Your function should rely on the SciPy's QR decomposition and triangular system solver.  Assume that $A$ is full rank.
You may test your function against the output of SciPy's least squares function, \li{linalg.lstsq()}.
\end{problem}
