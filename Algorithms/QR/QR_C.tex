\lab{Algorithms}{Modified Gram-Schmidt (QR)}{QR decomposition}
\label{lab:QRdecomp}

\objective{Understand how the QR algorithm works and write your own implementation.}

The QR decomposition is used to represent any square matrix as the matrix product of an orthogonal matrix, $Q$, and an upper triangular matrix, $R$.
This decomposition is useful in computing least squares and is part of a common method for finding eigenvalues (the QR algorithm).
We can succinctly state the QR decomposition in the following theorem.
\begin{theorem}
Let $A$ be an $m\times n$ matrix of rank $n$.  Then $A$ can be
factored into a product $Q R$, where $Q$ is an $m\times n$ matrix
with orthonormal columns and $R$ is a nonsingular $n \times n$ upper
triangular matrix.
\end{theorem}

\section*{Computing the QR Decomposition}
There are many methods for computing the QR factorization.
This lab will focus on the method that uses the Gram-Schmidt algorithm for orthogonalizing a matrix.
We use this algorithm to create $Q$.
If we let $\{\x_i\}_{i=1}^n$ be a basis for the inner product space $V$.
Let \[ \q_1 = \frac{\x_1}{\norm{\x_1}}\] and define $\q_2,\q_3,\ldots,\q_n$ recursively by
\[ \q_{k+1} = \x_{k+1} - \sum^k_{j=1}\frac{\langle \x_{k+1}, \q_j\rangle}{\norm{\q_{j}}^2}\q_j .\]
The sum term is a projection of $\x_{k+1}$ onto the subspace spanned by $\q_1,\q_2,\ldots,\q_k$.
Then the set $\{\q_i\}_{i=1}^n$ is an orthonormal basis for $V$.

For the above algorithm, let $r_{j k} = \langle \x_k, \q_j\rangle$ when $j \leq k$.
Then
\begin{align*}
r_{1 1} \q_1 &= \x_1\\
r_{k k} \q_k &= \x_k - r_{1 k} \q_1 - r_{2 k} \q_2 - r_{3 k} \q_3 -
\ldots - r_{k-1, k} \q_{k-1},\quad k=2,\ldots,n.
\end{align*}
This can be written as
\begin{align*}
\x_1 &= r_{1 1} \q_1\\
\x_2 &= r_{1 2} \q_1 + r_{2 2} \q_2\\
\vdots \:\: &= \quad \vdots\\
\x_n &= r_{1 n} \q_1 + r_{2 n} \q_2 + \ldots + r_{n n} \q_{n},
\end{align*}
or in matrix form as
\[
\begin{pmatrix}
\vdots & \vdots & & \vdots\\
\x_1 & \x_2 & \cdots & \x_n\\
\vdots & \vdots & & \vdots
\end{pmatrix}
=
\begin{pmatrix}
\vdots & \vdots & & \vdots\\
\q_1 & \q_2 & \cdots & \q_n\\
\vdots & \vdots & & \vdots
\end{pmatrix}
\begin{pmatrix}
r_{1 1} & r_{1 2} & \cdots & r_{1 n}\\
0 & r_{2 2} & \cdots & r_{2 n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & r_{n n}
\end{pmatrix}.
\]
Hence if our original basis $\{\x_i\}_{i=1}^n$ correspond to column
vectors of a matrix $A$, we can likewise write the resulting
orthonormal basis $\{\q_i\}_{i=1}^n$ as a matrix $Q$ of column
vectors.  Then we have that $A = Q R$, where $R$ is the above
nonsingular upper-triangular $n\times n$ matrix. 
Numerically, the Gram Schmidt process can have problems due to
finite precision arithmetic. Specifically, due to rounding errors,
the resulting basis may not be orthonormal. To combat this, we
actually carry out a slightly revised algorithm called Modified Gram
Schmidt.  To do this, we compute $\q_1$ as before.  We then project
it out of each of the remaining original vectors
$\x_2,\x_3,\ldots,\x_n$ via
\[
\x_k := \x_k - \langle \x_{k}, \q_1\rangle \q_1,\quad k=2,\ldots,n.
\]
Then we compute $\q_2$ to be the unit vector of $\x_2$, that is,
\[
\q_2 = \frac{\x_2}{\|\x_2\|}.
\]
We repeat by projecting out $\q_2$ from the remaining vectors
$\x_3,\x_4,\ldots,\x_n$.

\begin{problem}
\label{prob:QR}
Write your own implementation of the QR decomposition.  
It should accept as input a square matrix, $A$, and computes its QR decomposition, returning the matrices $Q$ and $R$ (which should be the same shape as $A$). 
Be sure to use the numerically stable Modified Gram-Schmidt algorithm.

You can test that you have the right decomposition by verifying that $QR=A$ and $Q^T Q = I$.
\end{problem}

\begin{problem}
The QR decomposition gives a really nice way to calculate the determinant of a square matrix.
Use your QR decomposition to calculate the determinant of any square matrix $A$.

Compare the determinant you calculate with the output of \li{linalg.det(A)}.
\end{problem}

\section*{QR Decomposition in SciPy}
The linear algebra library in SciPy wraps the very efficient algorithms of LAPACK to calculate the QR decomposition.
In addition to being much faster, SciPy's QR decomposition is also much more general and can decompose non-square matrices.
\begin{lstlisting}
>>> import numpy as np
>>> from scipy import linalg
>>> A = np.random.rand(4,3)
>>> Q, R = la.qr(A)
>>> Q.dot(R) == A     #there will be some False entries
>>> np.allclose(Q.dot(R), A) # A = QR
>>> np.allclose(Q.T.dot(Q), np.eye(4)) #Q is indeed, orthogonal
\end{lstlisting}

In order to interpret the results correctly, we need to understand that the computer has limited precision (especially with floating point numbers). 
This is why \li{Q.dot(R)} is not exactly equal to \li{A}.  
We can see that the matrix product of $QR$ is very close to $A$.
This shows that indeed the product of $Q$ and $R$ is $A$.  
Note also that $Q^T Q = I$.  
This implies that the column vectors of $Q$ are orthonormal.

\section*{Solving Least Squares Problems}
For large or ill-conditioned problems, the QR decomposition provides
a nice method for computing least squares solutions of
over-determined matrices.  Consider the problem $A x = b$.  Recall
that the least squares solution is $\widehat x = (A^T A)^{-1}A^T b$.
Alternatively, we write the linear system as
\[
Q R x = b.
\]
We then multiply both sides by $Q^T$, yielding
\[
R x = Q^T b.
\]
Then $\widehat x = R^{-1} Q^T b$.

\section*{}